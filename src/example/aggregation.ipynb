{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c216af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2182559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list loaded from ./llm_evaluation.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load data_list from a file\n",
    "def load_data_list(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        data_list = pickle.load(file)\n",
    "    print(f\"data_list loaded from {file_name}\")\n",
    "    return data_list\n",
    "\n",
    "# Example usage\n",
    "file_name = \"../llm_evaluation.pkl\"\n",
    "data_list = load_data_list(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355c89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from collections import defaultdict\n",
    "\n",
    "def custom_accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom accuracy function that considers predictions correct if they contain the same characters,\n",
    "    regardless of their order.\n",
    "\n",
    "    Args:\n",
    "        y_true (list): List of true labels.\n",
    "        y_pred (list): List of predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Custom accuracy score.\n",
    "    \"\"\"\n",
    "    correct = sum(sorted(true) == sorted(pred) for true, pred in zip(y_true, y_pred))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def custom_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom F1 score function that considers predictions correct if they contain the same characters,\n",
    "    regardless of their order.\n",
    "\n",
    "    Args:\n",
    "        y_true (list): List of true labels.\n",
    "        y_pred (list): List of predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Custom F1 score.\n",
    "    \"\"\"\n",
    "    y_true_sorted = [''.join(sorted(true)) for true in y_true]\n",
    "    y_pred_sorted = [''.join(sorted(pred)) for pred in y_pred]\n",
    "    precision = precision_score(y_true_sorted, y_pred_sorted, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true_sorted, y_pred_sorted, average='weighted', zero_division=0)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def compute_accuracy(data_list, vote_key):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of a specific voting method using custom accuracy score.\n",
    "\n",
    "    Args:\n",
    "        data_list (dict): A dictionary containing questions, answers, and voting results.\n",
    "        vote_key (str): The key in each entry representing the voting result to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The custom accuracy of the voting method.\n",
    "    \"\"\"\n",
    "    true_labels = [entry[\"answer\"] for entry in data_list.values()]\n",
    "    predicted_labels = [entry[vote_key] for entry in data_list.values()]\n",
    "    return custom_accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "def compute_grouped_accuracy(data_list, vote_key, group_key):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of a specific voting method grouped by a key using custom accuracy score.\n",
    "\n",
    "    Args:\n",
    "        data_list (dict): A dictionary containing questions, answers, and voting results.\n",
    "        vote_key (str): The key in each entry representing the voting result to evaluate.\n",
    "        group_key (str): The key in each entry to group by (e.g., 'domain' or 'q_type').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are group values and values are the custom accuracy for that group.\n",
    "    \"\"\"\n",
    "    grouped_data = defaultdict(list)\n",
    "    \n",
    "    # Group data by the specified key\n",
    "    for entry in data_list.values():\n",
    "        grouped_data[entry[group_key]].append(entry)\n",
    "    \n",
    "    # Compute custom accuracy for each group\n",
    "    grouped_accuracy = {}\n",
    "    for group, entries in grouped_data.items():\n",
    "        true_labels = [entry[\"answer\"] for entry in entries]\n",
    "        predicted_labels = [entry[vote_key] for entry in entries]\n",
    "        grouped_accuracy[group] = custom_accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    return grouped_accuracy\n",
    "\n",
    "aggregate_models = [\"chatglm3-6b-chat\", \"qwen2.5-7b-instruct\", \"baichuan2-7b-chat\", \"deepseek-v2-lite-chat\", \"hunyuan\"]\n",
    "\n",
    "# Filter data_list to include only aggregate_models\n",
    "filtered_data_list = {}\n",
    "for key, entry in data_list.items():\n",
    "    filtered_results = [\n",
    "        {model: result[model]} for result in entry[\"results\"] for model in result if model in aggregate_models\n",
    "    ]\n",
    "    filtered_entry = entry.copy()\n",
    "    filtered_entry[\"results\"] = filtered_results\n",
    "    filtered_data_list[key] = filtered_entry\n",
    "    \n",
    "def compute_f1_by_group(data_list, vote_key, group_key):\n",
    "    \"\"\"\n",
    "    Compute custom F1 scores for a specific voting method grouped by a key.\n",
    "\n",
    "    Args:\n",
    "        data_list (dict): A dictionary containing questions, answers, and voting results.\n",
    "        vote_key (str): The key in each entry representing the voting result to evaluate.\n",
    "        group_key (str): The key in each entry to group by (e.g., 'domain' or 'q_type').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are group values and values are the custom F1 scores for that group.\n",
    "    \"\"\"\n",
    "    grouped_data = defaultdict(list)\n",
    "    \n",
    "    # Group data by the specified key\n",
    "    for entry in data_list.values():\n",
    "        grouped_data[entry[group_key]].append(entry)\n",
    "    \n",
    "    # Compute custom F1 scores for each group\n",
    "    f1_scores = {}\n",
    "    for group, entries in grouped_data.items():\n",
    "        true_labels = [entry[\"answer\"] for entry in entries]\n",
    "        predicted_labels = [entry[vote_key] for entry in entries]\n",
    "        f1_scores[group] = custom_f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    return f1_scores\n",
    "\n",
    "def compute_accuracy_based_weights(data_list, models):\n",
    "    \"\"\"\n",
    "    Compute accuracy-based weights for models based on their agreement with the ground truth using custom comparison.\n",
    "\n",
    "    Args:\n",
    "        data_list (dict): A dictionary containing questions, answers, and model results.\n",
    "        models (list): A list of model names.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are model names and values are their accuracy-based weights.\n",
    "    \"\"\"\n",
    "    weights = {model: 0 for model in models}\n",
    "    total_labels = {model: 0 for model in models}\n",
    "\n",
    "    for q in data_list.values():\n",
    "        ground_truth = q[\"answer\"]\n",
    "        for model_result in q[\"results\"]:\n",
    "            for model, answer in model_result.items():\n",
    "                total_labels[model] += 1\n",
    "                if sorted(answer) == sorted(ground_truth):\n",
    "                    weights[model] += 1\n",
    "\n",
    "    # Normalize weights by dividing correct labels by total labels\n",
    "    for model in models:\n",
    "        weights[model] = weights[model] / total_labels[model] if total_labels[model] > 0 else 0\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24c251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(models_results):\n",
    "    \"\"\"\n",
    "    Perform majority voting to determine the most common answer.\n",
    "\n",
    "    Args:\n",
    "        models_results (list): A list of dictionaries containing model answers.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer with the highest vote count.\n",
    "    \"\"\"\n",
    "    # Extract the answers from the models' results\n",
    "    answers = [list(model.values())[0] for model in models_results]\n",
    "    # Count the occurrences of each answer\n",
    "    vote_counts = Counter(answers)\n",
    "    # Find the answer with the highest count (majority vote)\n",
    "    majority_answer = vote_counts.most_common(1)[0][0]\n",
    "    return majority_answer\n",
    "\n",
    "def weighted_majority_vote(models_results, accuracy_weights):\n",
    "    \"\"\"\n",
    "    Perform a weighted majority vote based on accuracy weights.\n",
    "\n",
    "    Args:\n",
    "        models_results (list): A list of dictionaries containing model answers.\n",
    "        accuracy_weights (dict): A dictionary with model names as keys and their accuracy weights as values.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer with the highest weighted vote.\n",
    "    \"\"\"\n",
    "    weighted_votes = Counter()\n",
    "\n",
    "    for model_result in models_results:\n",
    "        for model, answer in model_result.items():\n",
    "            # Add the weighted vote for the answer\n",
    "            weighted_votes[answer] += accuracy_weights.get(model, 0)\n",
    "\n",
    "    majority_answer = weighted_votes.most_common(1)[0][0]\n",
    "    return majority_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687bed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy (total): 0.5955601803676726\n",
      "Weighted Majority Vote Accuracy (total): 0.6398427563880217\n",
      "\n",
      "======== Accuracy Scores ========\n",
      "\n",
      "By Domain:\n",
      "\n",
      "Majority Vote: {'Production Safety': 0.5697709923664123, 'Oil and Gas': 0.6097256857855362, 'Fire Safety': 0.6624068157614483, 'Civil Engineering': 0.5338613861386139, 'Economics and Finance': 0.5951075495571488, 'Banking and Insurance': 0.6757512229210343}\n",
      "Weighted Majority Vote: {'Production Safety': 0.6178625954198473, 'Oil and Gas': 0.6327930174563591, 'Fire Safety': 0.6950656727014555, 'Civil Engineering': 0.5754455445544554, 'Economics and Finance': 0.649514972585407, 'Banking and Insurance': 0.7372466806429071}\n",
      "\n",
      "By Question Type:\n",
      "\n",
      "Majority Vote: {'Single Choice': 0.6524047138762077, 'Multiple Choice': 0.40807307012374777, 'True/False': 0.6180602006688963}\n",
      "Weighted Majority Vote: {'Single Choice': 0.6961460876950845, 'Multiple Choice': 0.5123747790218032, 'True/False': 0.6180602006688963}\n",
      "\n",
      "======== F1 Scores ========\n",
      "\n",
      "By Domain:\n",
      "\n",
      "Majority Vote: {'Production Safety': 0.5960662281172608, 'Oil and Gas': 0.6669376111293253, 'Fire Safety': 0.6634816758290113, 'Civil Engineering': 0.5378118833089637, 'Economics and Finance': 0.5994764446352493, 'Banking and Insurance': 0.6856804468181087}\n",
      "Weighted Majority Vote: {'Production Safety': 0.6445344944095356, 'Oil and Gas': 0.6903205066671131, 'Fire Safety': 0.7006160051425021, 'Civil Engineering': 0.5836454953311202, 'Economics and Finance': 0.6527261260516258, 'Banking and Insurance': 0.7459337736825712}\n",
      "\n",
      "By Question Type:\n",
      "\n",
      "Majority Vote: {'Single Choice': 0.6525838688147433, 'Multiple Choice': 0.44810122588324364, 'True/False': 0.6347831103958991}\n",
      "Weighted Majority Vote: {'Single Choice': 0.6974020670308583, 'Multiple Choice': 0.5351863875112797, 'True/False': 0.6347831103958991}\n"
     ]
    }
   ],
   "source": [
    "accuracy_weights = compute_accuracy_based_weights(filtered_data_list, aggregate_models)\n",
    "\n",
    "\n",
    "for q_id, entry in filtered_data_list.items():\n",
    "    # Perform majority voting\n",
    "    majority_result = majority_vote(entry[\"results\"])\n",
    "    entry[\"majority_vote\"] = majority_result\n",
    "\n",
    "    # Perform weighted majority voting\n",
    "    weighted_result = weighted_majority_vote(entry[\"results\"], accuracy_weights)\n",
    "    entry[\"weighted_majority_vote\"] = weighted_result\n",
    "\n",
    "# Compute accuracy for both methods\n",
    "majority_vote_accuracy = compute_accuracy(filtered_data_list, \"majority_vote\")\n",
    "weighted_majority_vote_accuracy = compute_accuracy(filtered_data_list, \"weighted_majority_vote\")\n",
    "\n",
    "print(\"Majority Vote Accuracy (total):\", majority_vote_accuracy)\n",
    "print(\"Weighted Majority Vote Accuracy (total):\", weighted_majority_vote_accuracy)\n",
    "\n",
    "print(\"\\n======== Accuracy Scores ========\\n\")\n",
    "\n",
    "# Compute accuracy grouped by domain\n",
    "majority_vote_accuracy_by_domain = compute_grouped_accuracy(filtered_data_list, \"majority_vote\", \"domain\")\n",
    "weighted_majority_vote_accuracy_by_domain = compute_grouped_accuracy(filtered_data_list, \"weighted_majority_vote\", \"domain\")\n",
    "\n",
    "# Compute accuracy grouped by question type\n",
    "majority_vote_accuracy_by_q_type = compute_grouped_accuracy(filtered_data_list, \"majority_vote\", \"q_type\")\n",
    "weighted_majority_vote_accuracy_by_q_type = compute_grouped_accuracy(filtered_data_list, \"weighted_majority_vote\", \"q_type\")\n",
    "print(\"By Domain:\\n\")\n",
    "print(\"Majority Vote:\", majority_vote_accuracy_by_domain)\n",
    "print(\"Weighted Majority Vote:\", weighted_majority_vote_accuracy_by_domain)\n",
    "\n",
    "print(\"\\nBy Question Type:\\n\")\n",
    "print(\"Majority Vote:\", majority_vote_accuracy_by_q_type)\n",
    "print(\"Weighted Majority Vote:\", weighted_majority_vote_accuracy_by_q_type)\n",
    "\n",
    "print(\"\\n======== F1 Scores ========\\n\")\n",
    "\n",
    "# Compute F1 scores grouped by domain\n",
    "majority_vote_f1_by_domain = compute_f1_by_group(filtered_data_list, \"majority_vote\", \"domain\")\n",
    "weighted_majority_vote_f1_by_domain = compute_f1_by_group(filtered_data_list, \"weighted_majority_vote\", \"domain\")\n",
    "# Compute F1 scores grouped by question type\n",
    "majority_vote_f1_by_q_type = compute_f1_by_group(filtered_data_list, \"majority_vote\", \"q_type\")\n",
    "weighted_majority_vote_f1_by_q_type = compute_f1_by_group(filtered_data_list, \"weighted_majority_vote\", \"q_type\")\n",
    "\n",
    "print(\"By Domain:\\n\")\n",
    "print(\"Majority Vote:\", majority_vote_f1_by_domain)\n",
    "print(\"Weighted Majority Vote:\", weighted_majority_vote_f1_by_domain)\n",
    "\n",
    "print(\"\\nBy Question Type:\\n\")\n",
    "print(\"Majority Vote:\", majority_vote_f1_by_q_type)\n",
    "print(\"Weighted Majority Vote:\", weighted_majority_vote_f1_by_q_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
