<div align="center">
  <a href="https://swarms.world">
    <h1>QualBenchğŸ’¯</h1>
  </a>
</div>
<p align="center">
  <em>Benchmarking Chinese LLMs with Localized Qualification Benchmark</em>
</p>

---

Qualification examinations in China are standardized, expert-verified tests designed to certify professionals across various domains, ensuring they meet industry and regulatory standards. These exams serve as critical gateways for workforce entry, offering a reliable framework for evaluating the domain-specific knowledge of LLMs in real-world applications.

We propose Qualification Benchmark (**QualBench**), a pioneering multi-domain Chinese QA benchmark designed to evaluate the performance of LLMs in localized Chinese context. Comprising 17,298 expert-verified questions sourced from 26 professional qualification examinations in China, the dataset addresses the gap in existing benchmarks, which often lack coverage of diverse vertical domains and fail to account for China-specific knowledge requirements.

<div style="font-size: 80%;">

| Dataset | Source Qualification Exam | Size | Best Model | Vertical Domain | Localization | Explainable |
|---|---------------------------------------------------------------|--------|------------|----------------|--------------|-------------|
| [GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench) | Chinese College Entrance Examination (Gaokao) | 2,811 | GPT-4 | âŒ | âœ… | âŒ |
| [CFLUE](https://github.com/aliyun/cflue) | Finance Qualification Exams | 38,636 | Qwen-72B | Finance | âŒ | âœ… |
| [M3KE](https://github.com/tjunlp-lab/M3KE) | Entrance Exams of Different Education Levels | 20,477 | GPT-3.5 | âŒ | âœ… | âŒ |
| [FinEval](https://github.com/SUFE-AIFLM-Lab/FinEval) | Finance Qualification Exams | 8,351 | GPT-4o | Finance | âŒ | âŒ |
| [CMExam](https://github.com/williamliujl/CMExam) | Chinese National Medical Licensing Exam | 68,119 | GPT-4 | Medical | âŒ | âŒ |
| [LogiQA](https://github.com/lgw863/LogiQA-dataset) | Civil Servants Exams of China | 8,678 | RoBERTa | âŒ | âœ… | âœ… |
| **QualBench (ours)** | Multiple Sources | 17,316 | Qwen-7B | Multiple | âœ… | âœ… |

</div>

## Dataset Statistics

<div style="display: flex; align-items: flex-start; gap: 20px; max-width: 800px; margin: 0 auto; justify-content: center;">
  <div style="font-size: 80%;">
    <table>
      <tr>
        <th><strong>Category</strong></th>
        <th><strong>Number of Questions</strong></th>
      </tr>
      <tr>
        <td>Production Safety (å®‰å…¨ç”Ÿäº§)</td>
        <td>6550</td>
      </tr>
      <tr>
        <td>Fire Safety (æ¶ˆé˜²å®‰å…¨)</td>
        <td>2817</td>
      </tr>
      <tr>
        <td>Civil Engineering (å»ºç­‘å·¥ç¨‹)</td>
        <td>2525</td>
      </tr>
      <tr>
        <td>Economics and Finance (ç»æµé‡‘è)</td>
        <td>2371</td>
      </tr>
      <tr>
        <td>Oil and Gas (çŸ³æ²¹å¤©ç„¶æ°”)</td>
        <td>1604</td>
      </tr>
      <tr>
        <td>Banking and Insurance (é“¶è¡Œä¿é™©)</td>
        <td>1431</td>
      </tr>
      <tr>
        <td><strong>Total</strong></td>
        <td>17,298</td>
      </tr>
    </table>
  </div>
  <div style="display: flex; flex-direction: column; gap: 10px;">
    <div>
      <img src="img/stats_by_domain.jpg" alt="Stats by Domain" style="width: 280px; height: auto;">
      <img src="img/stats_by_type.jpg" alt="Stats by Type" style="width: 250px; height: auto;">
    </div>
  </div>

</div>

## How to use QualBench

Load the data using Python's built-in `json` package as shown below:
```python
import json

file_path = "./QualBench_dataset.json"

with open(file_path, "r", encoding="utf8") as f:
    datas = json.load(f)
  
print(datas[0])

"""
{'idx': 0,
 'question_type': 'å•é€‰é¢˜',
 'domain': 'å®‰å…¨ç”Ÿäº§',
 'question': 'æœºæ¢°è®¾å¤‡è¿åŠ¨éƒ¨åˆ†æ˜¯æœ€å±é™©çš„éƒ¨ä½ï¼Œå°¤å…¶æ˜¯é‚£äº›æ“ä½œäººå‘˜æ˜“æ¥è§¦çš„é›¶éƒ¨ä»¶ï¼Œä¸‹åˆ—é’ˆå¯¹ä¸åŒæœºæ¢°è®¾å¤‡è½¬åŠ¨éƒ¨ä½çš„å±é™©æ‰€é‡‡å–çš„å®‰å…¨é˜²æŠ¤æªæ–½ä¸­ï¼Œæ­£ç¡®çš„æ˜¯ï¼ˆï¼‰ã€‚\nAï¼é’ˆå¯¹è½§é’¢æœºï¼Œåœ¨éªŒå¼æœºè½´å¤„é‡‡ç”¨é”¥å½¢é˜²æŠ¤ç½©é˜²æŠ¤\nBï¼é’ˆå¯¹è¾Šå¼è¾“é€æœºï¼Œåœ¨é©±åŠ¨è½´ä¸Šæ¸¸å®‰è£…é˜²æŠ¤ç½©é˜²æŠ¤\nCï¼é’ˆå¯¹å•®åˆé½¿è½®ï¼Œé½¿è½®è½¬åŠ¨æœºæ„é‡‡ç”¨åŠå°é—­é˜²æŠ¤\nDï¼é’ˆå¯¹æ‰‹æŒå¼ç ‚è½®æœºï¼Œåœ¨ç£¨å‰ŠåŒºé‡‡ç”¨å±€éƒ¨é˜²æŠ¤',
 'answer': 'A',
 'explanation': 'Bé¡¹ï¼Œè¾Šå¼è¾“é€æœºåº”è¯¥åœ¨é©±åŠ¨è½´çš„ä¸‹æ¸¸å®‰è£…é˜²æŠ¤ç½©ã€‚'}
"""
```

A complete code implementation is provided in `./test_gpt4o.py`.

## Evaluation Results
The benchmark evaluates five state-of-the-art Chinese LLMs (e.g., Qwen2.5, ChatGLM3) alongside non-Chinese models (e.g., GPT-4o, LLama-7B) in a one-shot setting. Results demonstrate that Chinese LLMs, particularly Qwen2.5, consistently outperform non-Chinese counterparts, highlighting the datasetâ€™s superiority in differentiating model capabilities in China-specific applications and emphasize the significant advantages of Chinese LLMs in applications under the Chinese context.

![Evaluation Result](img/fig_evaluation_result.jpg)
